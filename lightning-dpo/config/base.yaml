per_device_train_batch_size: 1
per_device_eval_batch_size: 1
num_train_epochs: 5
learning_rate: 2.e-5
weight_decay: 0.01
lr_scheduler_type: cosine
warmup_steps: 50
gradient_accumulation_steps: 1
max_grad_norm: 1.0
checkpointing_steps: 1.0